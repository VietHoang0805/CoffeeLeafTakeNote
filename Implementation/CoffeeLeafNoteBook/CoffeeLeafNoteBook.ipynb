{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Diseased Coffee Leaves Using Deep Learning\n",
    "\n",
    "\n",
    "In this project, given a set of images of coffee leaves, this project will explore deep learning algorithms (both fully connected and convolution\n",
    "neural networks) that output the correct labels for the conditions of the coffee leaves. \n",
    "\n",
    "The six conditions are \n",
    "* Healthy (H)\n",
    "* Rust Level 1 (RL1)\n",
    "* Rust Level 2 (RL2)\n",
    "* Rust Level 3 (RL3)\n",
    "* Rust Level 4 (RL4)\n",
    "* Red Spider Mites (RSM)\n",
    "\n",
    "For this project, we explore three different tasks:\n",
    "1) Given the full dataset, classify them into the 6 categories mentioned above.\n",
    "2) Given the full dataset, classify them into 3 categories (H, RL, RSM).\n",
    "3) Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.\n",
    "\n",
    "## Dataset\n",
    "**Robusta dataset**: [Dataset](https://drive.google.com/drive/folders/13fFAQHU_-Ar0zg6RHl1FTLOE3I2QnCWI?usp=sharing)\n",
    "\n",
    "\n",
    "### Setting the Virtual Environment and Installing Requirements\n",
    "Requirements:\n",
    "Run the follow commands:\n",
    "```sh\n",
    "$ pip install -r code/requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### To process the images for Task 1 above, run the following command:\n",
    "\n",
    "* [Data Processing](#dataprocessing)\n",
    "\n",
    "### To process the images for Task 2 above, run the following command:\n",
    "* [Data Processing For Three Class](#dataprocessingthreeclass)\n",
    "\n",
    "### To process the images for Task 3 above, run the following command:\n",
    "* [Data Processing For RegressionTask](#dataprocessingregression)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing <a class=\"anchor\" id=\"dataprocessing\"></a>\n",
    "This is code for data processing task\n",
    "\n",
    "**Given the full dataset, classify them into the 6 categories mentioned above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import collections\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "annotation_json_path = \"./Annotations/RoCoLe-json.json\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "binary_path = \"./binary/\"\n",
    "multiclass_path = \"./multiclass/\"\n",
    "\n",
    "binary_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"unhealthy\": 1\n",
    "}\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 2,\n",
    "    \"rust_level_3\": 3,\n",
    "    \"rust_level_4\": 4,\n",
    "    \"red_spider_mite\": 5\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict_old():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        binary_dict[binary].add(image_name)\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(binary_dict, multiclass_dict):\n",
    "    binary_split = split_into_train_val_test(binary_dict)\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return binary_split, multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, new_classification_path, classification_type, split_dict):\n",
    "    binary_or_multi = \"binary\" if \"binary\" in new_classification_path else \"multiclass\"\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"just_splitted\", binary_or_multi, split))\n",
    "            new_img_path = os.path.join(\"just_splitted\", binary_or_multi, split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "            # copyfile(os.path.join(\"just_splitted\", \"cropped\", img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    (binary_dict, multiclass_dict) = generate_binary_and_multiclass_dict_old()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for binary and multiclass classification\")\n",
    "    binary_split, multiclass_split = generate_train_val_test_split(binary_dict, multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    # copy_photo_files_into_directories(binary_dict, binary_path, binary_classifications, binary_split)\n",
    "    # if verbose:\n",
    "    #     print(\"Finished copying photos into the 'binary' folder\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_path, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def generate_binary_and_multiclass_dict(img_dimension = IMG_DIM):\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    make_dir(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\"))\n",
    "    with open(annotation_json_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        ct = 0\n",
    "        for pic_annotation in data:\n",
    "            if ct % 25 == 0: print(ct)\n",
    "            leaf_obj = pic_annotation[\"Label\"][\"Leaf\"][0]\n",
    "            geometry = leaf_obj[\"geometry\"]\n",
    "            img_name = pic_annotation[\"External ID\"]\n",
    "\n",
    "            binary_classif = leaf_obj[\"state\"]\n",
    "            multi_classif = pic_annotation[\"Label\"][\"classification\"]\n",
    "            classif_num = multiclass_classifications[multi_classif]\n",
    "\n",
    "            image = Image.open(os.path.join(photo_path_prefix, img_name))\n",
    "            width, height = image.size\n",
    "            midx = int(width/2)\n",
    "            midy = int(height/2)\n",
    "            img_dimension_temp = img_dimension * 2\n",
    "            zoom_cropped_img = image.crop((midx - img_dimension_temp, midy - img_dimension_temp, midx + img_dimension_temp, midy + img_dimension_temp))\n",
    "            zoom_cropped_img_name = str(classif_num) + \"_\" + img_name\n",
    "            zoom_cropped_img.save(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\", zoom_cropped_img_name))\n",
    "            binary_dict[binary_classif].add(zoom_cropped_img_name)\n",
    "            multiclass_dict[multi_classif].add(zoom_cropped_img_name)\n",
    "\n",
    "            # for i in range(len(geometry)):\n",
    "            #     xy = geometry[i]\n",
    "            #\n",
    "            #     x = xy[\"x\"]\n",
    "            #     y = xy[\"y\"]\n",
    "            #     xmin = x - img_dimension\n",
    "            #     xmax = x + img_dimension\n",
    "            #     ymin = y - img_dimension\n",
    "            #     ymax = y + img_dimension\n",
    "            #     if xmin < 0 or ymin < 0 or xmax > width or ymax > height:\n",
    "            #         continue\n",
    "            #\n",
    "            #     new_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "            #     new_img_name = str(classif_num) + \"_\" + \"{}_\".format(i) + img_name\n",
    "            #     new_img.save(os.path.join(\"cropped\", new_img_name))\n",
    "            #\n",
    "            #     binary_dict[binary_classif].add(new_img_name)\n",
    "            #     multiclass_dict[multi_classif].add(new_img_name)\n",
    "            ct += 1\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Three Class <a class=\"anchor\" id=\"dataprocessingthreeclass\"></a>\n",
    "This is code for data processing for three class\n",
    "\n",
    "**Given the full dataset, classify them into 3 categories (H, RL, RSM).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import collections\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 1,\n",
    "    \"rust_level_3\": 1,\n",
    "    \"rust_level_4\": 1,\n",
    "    \"red_spider_mite\": 2\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(multiclass_dict):\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, classification_type, split_dict):\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"three_classes\", \"multiclass\", split))\n",
    "            new_img_path = os.path.join(\"three_classes\", \"multiclass\", split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    multiclass_dict = generate_binary_and_multiclass_dict()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for multiclass classification\")\n",
    "    multiclass_split = generate_train_val_test_split(multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Regression Task <a class=\"anchor\" id=\"dataprocessingregression\"></a>\n",
    "This is code for data processing for Regression Task\n",
    "\n",
    "**Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
