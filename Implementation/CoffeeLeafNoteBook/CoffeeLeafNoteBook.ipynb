{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Library\n",
    "import openpyxl\n",
    "import collections\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate\n",
    "import loss_and_metrics\n",
    "\n",
    "import model.regression_adopted_cnn as regression_cnn\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate\n",
    "import regression_loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Diseased Coffee Leaves Using Deep Learning\n",
    "\n",
    "\n",
    "In this project, given a set of images of coffee leaves, this project will explore deep learning algorithms (both fully connected and convolution\n",
    "neural networks) that output the correct labels for the conditions of the coffee leaves. \n",
    "\n",
    "The six conditions are \n",
    "* Healthy (H)\n",
    "* Rust Level 1 (RL1)\n",
    "* Rust Level 2 (RL2)\n",
    "* Rust Level 3 (RL3)\n",
    "* Rust Level 4 (RL4)\n",
    "* Red Spider Mites (RSM)\n",
    "\n",
    "For this project, we explore three different tasks:\n",
    "1) Given the full dataset, classify them into the 6 categories mentioned above.\n",
    "2) Given the full dataset, classify them into 3 categories (H, RL, RSM).\n",
    "3) Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.\n",
    "\n",
    "## Dataset\n",
    "**Robusta dataset**: [Dataset](https://drive.google.com/drive/folders/13fFAQHU_-Ar0zg6RHl1FTLOE3I2QnCWI?usp=sharing)\n",
    "\n",
    "\n",
    "### Setting the Virtual Environment and Installing Requirements\n",
    "Requirements:\n",
    "Run the follow commands:\n",
    "```sh\n",
    "$ pip install -r code/requirements.txt\n",
    "```\n",
    "Processing the Dataset\n",
    "After downloading the annotations and images, they should be placed inside the CoffeeLeafNoteBook directory as follows\n",
    "\n",
    "CoffeeLeafNoteBook/Annotations/{annotation files}\n",
    "CoffeeLeafNoteBook/Photos/{.jpg files}\n",
    "\n",
    "### To process the images for Task 1 above, run the following command:\n",
    "\n",
    "* [Data Processing](#dataprocessing)\n",
    "\n",
    "### To process the images for Task 2 above, run the following command:\n",
    "* [Data Processing For Three Class](#dataprocessingthreeclass)\n",
    "\n",
    "### To process the images for Task 3 above, run the following command:\n",
    "* [Data Processing For RegressionTask](#dataprocessingregression)\n",
    "\n",
    "### Training models\n",
    "To train models, first create a ```params.json``` file inside the ```experiments/{A}/{B}``` directory, where\n",
    "* {A} is either ```six_classes```, ```three_classes```, or ```regression```\n",
    "* {B} is the descriptive name for the experiment model\n",
    "\n",
    "**Then for Task 1 and 2, we use [Training the model](#trainmodel)**\n",
    "\n",
    "**For Task 3, we use the [Train Regression](#trainmodelregression)**\n",
    "\n",
    "### Evaluate on a Saved Model\n",
    "**To evaluate on a saved model, run [Evaluate model](#evaluatef1)**\n",
    "\n",
    "\n",
    "By default, it will evaluate on only the training and validation set.\n",
    "\n",
    "+ To evaluate on the test set, the --testSet True flag must be added.\n",
    "+ To not evaluate on the training and validation set, we can set the --trainAndVal False.\n",
    "+ To evaluate only on the test set, we can set both flags --trainAndVal False --testSet True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing <a class=\"anchor\" id=\"dataprocessing\"></a>\n",
    "This is code for data processing task\n",
    "\n",
    "**Given the full dataset, classify them into the 6 categories mentioned above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for binary and multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "annotation_json_path = \"./Annotations/RoCoLe-json.json\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "binary_path = \"./binary/\"\n",
    "multiclass_path = \"./multiclass/\"\n",
    "\n",
    "binary_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"unhealthy\": 1\n",
    "}\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 2,\n",
    "    \"rust_level_3\": 3,\n",
    "    \"rust_level_4\": 4,\n",
    "    \"red_spider_mite\": 5\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict_old():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        binary_dict[binary].add(image_name)\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(binary_dict, multiclass_dict):\n",
    "    binary_split = split_into_train_val_test(binary_dict)\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return binary_split, multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, new_classification_path, classification_type, split_dict):\n",
    "    binary_or_multi = \"binary\" if \"binary\" in new_classification_path else \"multiclass\"\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"just_splitted\", binary_or_multi, split))\n",
    "            new_img_path = os.path.join(\"just_splitted\", binary_or_multi, split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "            # copyfile(os.path.join(\"just_splitted\", \"cropped\", img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    (binary_dict, multiclass_dict) = generate_binary_and_multiclass_dict_old()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for binary and multiclass classification\")\n",
    "    binary_split, multiclass_split = generate_train_val_test_split(binary_dict, multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    # copy_photo_files_into_directories(binary_dict, binary_path, binary_classifications, binary_split)\n",
    "    # if verbose:\n",
    "    #     print(\"Finished copying photos into the 'binary' folder\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_path, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def generate_binary_and_multiclass_dict(img_dimension = IMG_DIM):\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    make_dir(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\"))\n",
    "    with open(annotation_json_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        ct = 0\n",
    "        for pic_annotation in data:\n",
    "            if ct % 25 == 0: print(ct)\n",
    "            leaf_obj = pic_annotation[\"Label\"][\"Leaf\"][0]\n",
    "            geometry = leaf_obj[\"geometry\"]\n",
    "            img_name = pic_annotation[\"External ID\"]\n",
    "\n",
    "            binary_classif = leaf_obj[\"state\"]\n",
    "            multi_classif = pic_annotation[\"Label\"][\"classification\"]\n",
    "            classif_num = multiclass_classifications[multi_classif]\n",
    "\n",
    "            image = Image.open(os.path.join(photo_path_prefix, img_name))\n",
    "            width, height = image.size\n",
    "            midx = int(width/2)\n",
    "            midy = int(height/2)\n",
    "            img_dimension_temp = img_dimension * 2\n",
    "            zoom_cropped_img = image.crop((midx - img_dimension_temp, midy - img_dimension_temp, midx + img_dimension_temp, midy + img_dimension_temp))\n",
    "            zoom_cropped_img_name = str(classif_num) + \"_\" + img_name\n",
    "            zoom_cropped_img.save(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\", zoom_cropped_img_name))\n",
    "            binary_dict[binary_classif].add(zoom_cropped_img_name)\n",
    "            multiclass_dict[multi_classif].add(zoom_cropped_img_name)\n",
    "\n",
    "            # for i in range(len(geometry)):\n",
    "            #     xy = geometry[i]\n",
    "            #\n",
    "            #     x = xy[\"x\"]\n",
    "            #     y = xy[\"y\"]\n",
    "            #     xmin = x - img_dimension\n",
    "            #     xmax = x + img_dimension\n",
    "            #     ymin = y - img_dimension\n",
    "            #     ymax = y + img_dimension\n",
    "            #     if xmin < 0 or ymin < 0 or xmax > width or ymax > height:\n",
    "            #         continue\n",
    "            #\n",
    "            #     new_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "            #     new_img_name = str(classif_num) + \"_\" + \"{}_\".format(i) + img_name\n",
    "            #     new_img.save(os.path.join(\"cropped\", new_img_name))\n",
    "            #\n",
    "            #     binary_dict[binary_classif].add(new_img_name)\n",
    "            #     multiclass_dict[multi_classif].add(new_img_name)\n",
    "            ct += 1\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Three Class <a class=\"anchor\" id=\"dataprocessingthreeclass\"></a>\n",
    "This is code for data processing for three class\n",
    "\n",
    "**Given the full dataset, classify them into 3 categories (H, RL, RSM).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 1,\n",
    "    \"rust_level_3\": 1,\n",
    "    \"rust_level_4\": 1,\n",
    "    \"red_spider_mite\": 2\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(multiclass_dict):\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, classification_type, split_dict):\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"three_classes\", \"multiclass\", split))\n",
    "            new_img_path = os.path.join(\"three_classes\", \"multiclass\", split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    multiclass_dict = generate_binary_and_multiclass_dict()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for multiclass classification\")\n",
    "    multiclass_split = generate_train_val_test_split(multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Regression Task <a class=\"anchor\" id=\"dataprocessingregression\"></a>\n",
    "This is code for data processing for Regression Task\n",
    "\n",
    "**Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 2,\n",
    "    \"rust_level_3\": 3,\n",
    "    \"rust_level_4\": 4,\n",
    "    \"red_spider_mite\": 5\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(multiclass_dict):\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, classification_type, split_dict):\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = classification_type[category]\n",
    "        if num == 5: continue\n",
    "        num_str = str(num)\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"regression\", \"multiclass\", split))\n",
    "            new_img_path = os.path.join(\"regression\", \"multiclass\", split, num_str + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    multiclass_dict = generate_binary_and_multiclass_dict()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for multiclass classification\")\n",
    "    multiclass_split = generate_train_val_test_split(multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model For Task 1 and 2 <a class=\"anchor\" id=\"trainmodel\"></a>\n",
    "This is code for training the model for task 1 and task 2\n",
    "\n",
    "+ Task 1: Given the full dataset, classify them into the 6 categories mentioned above.\n",
    "+ Task 2: Given the full dataset, classify them into 3 categories (H, RL, RSM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Choose model to train example mobilenet_baseline on six class\n",
    "# !python train.py --model_dir experiments/six_classes/mobilenet_baseline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model For Task 3 <a class=\"anchor\" id=\"trainmodelregression\"></a>\n",
    "This is code training the model for task 3 \n",
    "\n",
    "**Task 3: Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Import your utility functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m  \u001b[38;5;66;03m# Ensure utils is available in your notebook environment\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mloss_and_metrics\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set the arguments manually instead of using argparse\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_loader'"
     ]
    }
   ],
   "source": [
    "!python train.py --model_dir experiments/regression/mobilenet_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluated Model <a class=\"anchor\" id=\"evaluatef1\"></a>\n",
    "This is code evaluated model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can evaluate F1 score model after train example mobilenet_baseline on six classes\n",
    "python calculateF1Metrics --model_dir experiments/six_classes/mobilenet_baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
