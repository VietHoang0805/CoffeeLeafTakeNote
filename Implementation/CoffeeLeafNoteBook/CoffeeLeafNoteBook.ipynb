{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## Import Library\n",
    "import openpyxl\n",
    "import collections\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate\n",
    "import loss_and_metrics\n",
    "\n",
    "import model.regression_adopted_cnn as regression_cnn\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate\n",
    "import regression_loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Diseased Coffee Leaves Using Deep Learning\n",
    "\n",
    "\n",
    "In this project, given a set of images of coffee leaves, this project will explore deep learning algorithms (both fully connected and convolution\n",
    "neural networks) that output the correct labels for the conditions of the coffee leaves. \n",
    "\n",
    "The six conditions are \n",
    "* Healthy (H)\n",
    "* Rust Level 1 (RL1)\n",
    "* Rust Level 2 (RL2)\n",
    "* Rust Level 3 (RL3)\n",
    "* Rust Level 4 (RL4)\n",
    "* Red Spider Mites (RSM)\n",
    "\n",
    "For this project, we explore three different tasks:\n",
    "1) Given the full dataset, classify them into the 6 categories mentioned above.\n",
    "2) Given the full dataset, classify them into 3 categories (H, RL, RSM).\n",
    "3) Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.\n",
    "\n",
    "## Dataset\n",
    "**Robusta dataset**: [Dataset](https://drive.google.com/drive/folders/13fFAQHU_-Ar0zg6RHl1FTLOE3I2QnCWI?usp=sharing)\n",
    "\n",
    "\n",
    "### Setting the Virtual Environment and Installing Requirements\n",
    "Requirements:\n",
    "Run the follow commands:\n",
    "```sh\n",
    "$ pip install -r code/requirements.txt\n",
    "```\n",
    "Processing the Dataset\n",
    "After downloading the annotations and images, they should be placed inside the CoffeeLeafNoteBook directory as follows\n",
    "\n",
    "CoffeeLeafNoteBook/Annotations/{annotation files}\n",
    "CoffeeLeafNoteBook/Photos/{.jpg files}\n",
    "\n",
    "### To process the images for Task 1 above, run the following command:\n",
    "\n",
    "* [Data Processing](#dataprocessing)\n",
    "\n",
    "### To process the images for Task 2 above, run the following command:\n",
    "* [Data Processing For Three Class](#dataprocessingthreeclass)\n",
    "\n",
    "### To process the images for Task 3 above, run the following command:\n",
    "* [Data Processing For RegressionTask](#dataprocessingregression)\n",
    "\n",
    "### Training models\n",
    "To train models, first create a ```params.json``` file inside the ```experiments/{A}/{B}``` directory, where\n",
    "* {A} is either ```six_classes```, ```three_classes```, or ```regression```\n",
    "* {B} is the descriptive name for the experiment model\n",
    "\n",
    "**Then for Task 1 and 2, we use [Training the model](#trainmodel)**\n",
    "\n",
    "**For Task 3, we use the [Train Regression](#trainmodelregression)**\n",
    "\n",
    "### Evaluate on a Saved Model\n",
    "**To evaluate on a saved model, run [Evaluate model](#evaluatef1)**\n",
    "\n",
    "\n",
    "By default, it will evaluate on only the training and validation set.\n",
    "\n",
    "+ To evaluate on the test set, the --testSet True flag must be added.\n",
    "+ To not evaluate on the training and validation set, we can set the --trainAndVal False.\n",
    "+ To evaluate only on the test set, we can set both flags --trainAndVal False --testSet True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing <a class=\"anchor\" id=\"dataprocessing\"></a>\n",
    "This is code for data processing task\n",
    "\n",
    "**Given the full dataset, classify them into the 6 categories mentioned above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for binary and multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "annotation_json_path = \"./Annotations/RoCoLe-json.json\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "binary_path = \"./binary/\"\n",
    "multiclass_path = \"./multiclass/\"\n",
    "\n",
    "binary_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"unhealthy\": 1\n",
    "}\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 2,\n",
    "    \"rust_level_3\": 3,\n",
    "    \"rust_level_4\": 4,\n",
    "    \"red_spider_mite\": 5\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict_old():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        binary_dict[binary].add(image_name)\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(binary_dict, multiclass_dict):\n",
    "    binary_split = split_into_train_val_test(binary_dict)\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return binary_split, multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, new_classification_path, classification_type, split_dict):\n",
    "    binary_or_multi = \"binary\" if \"binary\" in new_classification_path else \"multiclass\"\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"just_splitted\", binary_or_multi, split))\n",
    "            new_img_path = os.path.join(\"just_splitted\", binary_or_multi, split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "            # copyfile(os.path.join(\"just_splitted\", \"cropped\", img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    (binary_dict, multiclass_dict) = generate_binary_and_multiclass_dict_old()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for binary and multiclass classification\")\n",
    "    binary_split, multiclass_split = generate_train_val_test_split(binary_dict, multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    # copy_photo_files_into_directories(binary_dict, binary_path, binary_classifications, binary_split)\n",
    "    # if verbose:\n",
    "    #     print(\"Finished copying photos into the 'binary' folder\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_path, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def generate_binary_and_multiclass_dict(img_dimension = IMG_DIM):\n",
    "    binary_dict = collections.defaultdict(set)\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    make_dir(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\"))\n",
    "    with open(annotation_json_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        ct = 0\n",
    "        for pic_annotation in data:\n",
    "            if ct % 25 == 0: print(ct)\n",
    "            leaf_obj = pic_annotation[\"Label\"][\"Leaf\"][0]\n",
    "            geometry = leaf_obj[\"geometry\"]\n",
    "            img_name = pic_annotation[\"External ID\"]\n",
    "\n",
    "            binary_classif = leaf_obj[\"state\"]\n",
    "            multi_classif = pic_annotation[\"Label\"][\"classification\"]\n",
    "            classif_num = multiclass_classifications[multi_classif]\n",
    "\n",
    "            image = Image.open(os.path.join(photo_path_prefix, img_name))\n",
    "            width, height = image.size\n",
    "            midx = int(width/2)\n",
    "            midy = int(height/2)\n",
    "            img_dimension_temp = img_dimension * 2\n",
    "            zoom_cropped_img = image.crop((midx - img_dimension_temp, midy - img_dimension_temp, midx + img_dimension_temp, midy + img_dimension_temp))\n",
    "            zoom_cropped_img_name = str(classif_num) + \"_\" + img_name\n",
    "            zoom_cropped_img.save(os.path.join(\"zoom_cropped_and_splitted\", \"cropped\", zoom_cropped_img_name))\n",
    "            binary_dict[binary_classif].add(zoom_cropped_img_name)\n",
    "            multiclass_dict[multi_classif].add(zoom_cropped_img_name)\n",
    "\n",
    "            # for i in range(len(geometry)):\n",
    "            #     xy = geometry[i]\n",
    "            #\n",
    "            #     x = xy[\"x\"]\n",
    "            #     y = xy[\"y\"]\n",
    "            #     xmin = x - img_dimension\n",
    "            #     xmax = x + img_dimension\n",
    "            #     ymin = y - img_dimension\n",
    "            #     ymax = y + img_dimension\n",
    "            #     if xmin < 0 or ymin < 0 or xmax > width or ymax > height:\n",
    "            #         continue\n",
    "            #\n",
    "            #     new_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "            #     new_img_name = str(classif_num) + \"_\" + \"{}_\".format(i) + img_name\n",
    "            #     new_img.save(os.path.join(\"cropped\", new_img_name))\n",
    "            #\n",
    "            #     binary_dict[binary_classif].add(new_img_name)\n",
    "            #     multiclass_dict[multi_classif].add(new_img_name)\n",
    "            ct += 1\n",
    "\n",
    "    return binary_dict, multiclass_dict\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Three Class <a class=\"anchor\" id=\"dataprocessingthreeclass\"></a>\n",
    "This is code for data processing for three class\n",
    "\n",
    "**Given the full dataset, classify them into 3 categories (H, RL, RSM).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 1,\n",
    "    \"rust_level_3\": 1,\n",
    "    \"rust_level_4\": 1,\n",
    "    \"red_spider_mite\": 2\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        binary = sheet_obj.cell(row=i, column=2).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(multiclass_dict):\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, classification_type, split_dict):\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = str(classification_type[category])\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"three_classes\", \"multiclass\", split))\n",
    "            new_img_path = os.path.join(\"three_classes\", \"multiclass\", split, num + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    multiclass_dict = generate_binary_and_multiclass_dict()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for multiclass classification\")\n",
    "    multiclass_split = generate_train_val_test_split(multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing For Regression Task <a class=\"anchor\" id=\"dataprocessingregression\"></a>\n",
    "This is code for data processing for Regression Task\n",
    "\n",
    "**Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished categorizing pictures into their respective classes for multiclass classification\n",
      "Finished splitting dataset\n",
      "Finished copying photos into the 'multiclass' folder\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 720\n",
    "xlsx_path = \"./Annotations/RoCoLe-classes.xlsx\"\n",
    "photo_path_prefix = \"./Photos/\"\n",
    "\n",
    "\n",
    "multiclass_classifications = {\n",
    "    \"healthy\": 0,\n",
    "    \"rust_level_1\": 1,\n",
    "    \"rust_level_2\": 2,\n",
    "    \"rust_level_3\": 3,\n",
    "    \"rust_level_4\": 4,\n",
    "    \"red_spider_mite\": 5\n",
    "}\n",
    "\n",
    "def split_into_train_val_test(dict):\n",
    "    random.seed(230)\n",
    "\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "\n",
    "    for category in dict:\n",
    "        img_names = list(dict[category])\n",
    "        img_names.sort()\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        test_split = int(0.1 * len(img_names))\n",
    "        val_split = int(.18 * len(img_names))\n",
    "\n",
    "        test_img_names = img_names[:test_split]\n",
    "        val_img_names = img_names[test_split: test_split + val_split]\n",
    "        train_img_names = img_names[test_split + val_split:]\n",
    "\n",
    "        test.extend(test_img_names)\n",
    "        val.extend(val_img_names)\n",
    "        train.extend(train_img_names)\n",
    "\n",
    "    return {\n",
    "        \"test\": set(test),\n",
    "        \"val\": set(val),\n",
    "        \"train\": set(train)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_binary_and_multiclass_dict():\n",
    "    wb_obj = openpyxl.load_workbook(xlsx_path)\n",
    "    sheet_obj = wb_obj.active\n",
    "\n",
    "    multiclass_dict = collections.defaultdict(set)\n",
    "\n",
    "    num_row = sheet_obj.max_row\n",
    "\n",
    "    for i in range(2, num_row + 1):\n",
    "        image_name = sheet_obj.cell(row=i, column=1).value\n",
    "        multiclass = sheet_obj.cell(row=i, column=3).value\n",
    "\n",
    "        multiclass_dict[multiclass].add(image_name)\n",
    "\n",
    "    return multiclass_dict\n",
    "\n",
    "\n",
    "def generate_train_val_test_split(multiclass_dict):\n",
    "    multiclass_split = split_into_train_val_test(multiclass_dict)\n",
    "    return multiclass_split\n",
    "\n",
    "\n",
    "def get_split(img, split_dict):\n",
    "    if img in split_dict[\"test\"]:\n",
    "        return \"test\"\n",
    "    if img in split_dict[\"val\"]:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def resize_and_save(filename, output_path, size=IMG_DIM):\n",
    "    \"\"\"Resize the image contained in `filename` and save it to the `output_dir`\"\"\"\n",
    "    image = Image.open(filename)\n",
    "    # Use bilinear interpolation instead of the default \"nearest neighbor\" method\n",
    "    image = image.resize((size, size), Image.BILINEAR)\n",
    "    image.save(output_path)\n",
    "\n",
    "\n",
    "def copy_photo_files_into_directories(classification_dict, classification_type, split_dict):\n",
    "    for (category, images) in classification_dict.items():\n",
    "        num = classification_type[category]\n",
    "        if num == 5: continue\n",
    "        num_str = str(num)\n",
    "        for img in images:\n",
    "            split = get_split(img, split_dict)\n",
    "            make_dir(os.path.join(\"regression\", \"multiclass\", split))\n",
    "            new_img_path = os.path.join(\"regression\", \"multiclass\", split, num_str + \"_\" + img)\n",
    "            resize_and_save(os.path.join(photo_path_prefix, img), new_img_path)\n",
    "\n",
    "\n",
    "def categorize_train_val_test_split(verbose = False):\n",
    "    multiclass_dict = generate_binary_and_multiclass_dict()\n",
    "    if verbose:\n",
    "        print(\"Finished categorizing pictures into their respective classes for multiclass classification\")\n",
    "    multiclass_split = generate_train_val_test_split(multiclass_dict)\n",
    "    if verbose:\n",
    "        print(\"Finished splitting dataset\")\n",
    "    copy_photo_files_into_directories(multiclass_dict, multiclass_classifications, multiclass_split)\n",
    "    if verbose:\n",
    "        print(\"Finished copying photos into the 'multiclass' folder\")\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    path = os.path.abspath(os.path.join(path))\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except Exception as e:\n",
    "            # Raise if directory can't be made, because image cuts won't be saved.\n",
    "            print('Error creating directory')\n",
    "            raise e\n",
    "\n",
    "def main():\n",
    "    categorize_train_val_test_split(True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model For Task 1 and 2 <a class=\"anchor\" id=\"trainmodel\"></a>\n",
    "This is code for training the model for task 1 and task 2\n",
    "\n",
    "+ Task 1: Given the full dataset, classify them into the 6 categories mentioned above.\n",
    "+ Task 2: Given the full dataset, classify them into 3 categories (H, RL, RSM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2906995662.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    python train.py --model_dir experiments/six_classes/mobilenet_baseline/\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Choose model to train example mobilenet_baseline on six class\n",
    "!python train.py --model_dir experiments/six_classes/mobilenet_baseline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model For Task 3 <a class=\"anchor\" id=\"trainmodelregression\"></a>\n",
    "This is code training the model for task 3 \n",
    "\n",
    "**Task 3: Given the images from the healthy and rust level categories only, classify them into 5 categories (H, RL1, RL2, RL3, RL4) using a regression-based approach.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_dir experiments/regression/mobilenet_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluated Model <a class=\"anchor\" id=\"evaluatef1\"></a>\n",
    "This is code evaluated model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Creating the dataset...\n",
      "- done.\n",
      "Starting evaluation and calculation of F1 Scores\n",
      "h:\\CoffeeLeafTakeNote\\Implementation\\CoffeeLeafNoteBook\\utils.py:174: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint)\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "- Train Metrics : accuracy: 0.985 ; macro f1: 0.964 ; macro precision: 0.965 ; macro recall: 0.969 ; loss: 0.029\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "- Val Metrics : accuracy: 0.762 ; macro f1: 0.541 ; macro precision: 0.578 ; macro recall: 0.523 ; loss: 1.146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: 6 for 1128 total pictures\n"
     ]
    }
   ],
   "source": [
    "# Can evaluate F1 score model after train example mobilenet_baseline on six classes\n",
    "!python calculateF1Metrics.py --model_dir experiments/six_classes/mobilenet_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thử nghiệm dự đoán bệnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Viet/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "h:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Viet\\AppData\\Local\\Temp\\ipykernel_4044\\1219257838.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNet:\n\tMissing key(s) in state_dict: \"model.features.0.0.weight\", \"model.features.0.1.weight\", \"model.features.0.1.bias\", \"model.features.0.1.running_mean\", \"model.features.0.1.running_var\", \"model.features.1.conv.0.0.weight\", \"model.features.1.conv.0.1.weight\", \"model.features.1.conv.0.1.bias\", \"model.features.1.conv.0.1.running_mean\", \"model.features.1.conv.0.1.running_var\", \"model.features.1.conv.1.weight\", \"model.features.1.conv.2.weight\", \"model.features.1.conv.2.bias\", \"model.features.1.conv.2.running_mean\", \"model.features.1.conv.2.running_var\", \"model.features.2.conv.0.0.weight\", \"model.features.2.conv.0.1.weight\", \"model.features.2.conv.0.1.bias\", \"model.features.2.conv.0.1.running_mean\", \"model.features.2.conv.0.1.running_var\", \"model.features.2.conv.1.0.weight\", \"model.features.2.conv.1.1.weight\", \"model.features.2.conv.1.1.bias\", \"model.features.2.conv.1.1.running_mean\", \"model.features.2.conv.1.1.running_var\", \"model.features.2.conv.2.weight\", \"model.features.2.conv.3.weight\", \"model.features.2.conv.3.bias\", \"model.features.2.conv.3.running_mean\", \"model.features.2.conv.3.running_var\", \"model.features.3.conv.0.0.weight\", \"model.features.3.conv.0.1.weight\", \"model.features.3.conv.0.1.bias\", \"model.features.3.conv.0.1.running_mean\", \"model.features.3.conv.0.1.running_var\", \"model.features.3.conv.1.0.weight\", \"model.features.3.conv.1.1.weight\", \"model.features.3.conv.1.1.bias\", \"model.features.3.conv.1.1.running_mean\", \"model.features.3.conv.1.1.running_var\", \"model.features.3.conv.2.weight\", \"model.features.3.conv.3.weight\", \"model.features.3.conv.3.bias\", \"model.features.3.conv.3.running_mean\", \"model.features.3.conv.3.running_var\", \"model.features.4.conv.0.0.weight\", \"model.features.4.conv.0.1.weight\", \"model.features.4.conv.0.1.bias\", \"model.features.4.conv.0.1.running_mean\", \"model.features.4.conv.0.1.running_var\", \"model.features.4.conv.1.0.weight\", \"model.features.4.conv.1.1.weight\", \"model.features.4.conv.1.1.bias\", \"model.features.4.conv.1.1.running_mean\", \"model.features.4.conv.1.1.running_var\", \"model.features.4.conv.2.weight\", \"model.features.4.conv.3.weight\", \"model.features.4.conv.3.bias\", \"model.features.4.conv.3.running_mean\", \"model.features.4.conv.3.running_var\", \"model.features.5.conv.0.0.weight\", \"model.features.5.conv.0.1.weight\", \"model.features.5.conv.0.1.bias\", \"model.features.5.conv.0.1.running_mean\", \"model.features.5.conv.0.1.running_var\", \"model.features.5.conv.1.0.weight\", \"model.features.5.conv.1.1.weight\", \"model.features.5.conv.1.1.bias\", \"model.features.5.conv.1.1.running_mean\", \"model.features.5.conv.1.1.running_var\", \"model.features.5.conv.2.weight\", \"model.features.5.conv.3.weight\", \"model.features.5.conv.3.bias\", \"model.features.5.conv.3.running_mean\", \"model.features.5.conv.3.running_var\", \"model.features.6.conv.0.0.weight\", \"model.features.6.conv.0.1.weight\", \"model.features.6.conv.0.1.bias\", \"model.features.6.conv.0.1.running_mean\", \"model.features.6.conv.0.1.running_var\", \"model.features.6.conv.1.0.weight\", \"model.features.6.conv.1.1.weight\", \"model.features.6.conv.1.1.bias\", \"model.features.6.conv.1.1.running_mean\", \"model.features.6.conv.1.1.running_var\", \"model.features.6.conv.2.weight\", \"model.features.6.conv.3.weight\", \"model.features.6.conv.3.bias\", \"model.features.6.conv.3.running_mean\", \"model.features.6.conv.3.running_var\", \"model.features.7.conv.0.0.weight\", \"model.features.7.conv.0.1.weight\", \"model.features.7.conv.0.1.bias\", \"model.features.7.conv.0.1.running_mean\", \"model.features.7.conv.0.1.running_var\", \"model.features.7.conv.1.0.weight\", \"model.features.7.conv.1.1.weight\", \"model.features.7.conv.1.1.bias\", \"model.features.7.conv.1.1.running_mean\", \"model.features.7.conv.1.1.running_var\", \"model.features.7.conv.2.weight\", \"model.features.7.conv.3.weight\", \"model.features.7.conv.3.bias\", \"model.features.7.conv.3.running_mean\", \"model.features.7.conv.3.running_var\", \"model.features.8.conv.0.0.weight\", \"model.features.8.conv.0.1.weight\", \"model.features.8.conv.0.1.bias\", \"model.features.8.conv.0.1.running_mean\", \"model.features.8.conv.0.1.running_var\", \"model.features.8.conv.1.0.weight\", \"model.features.8.conv.1.1.weight\", \"model.features.8.conv.1.1.bias\", \"model.features.8.conv.1.1.running_mean\", \"model.features.8.conv.1.1.running_var\", \"model.features.8.conv.2.weight\", \"model.features.8.conv.3.weight\", \"model.features.8.conv.3.bias\", \"model.features.8.conv.3.running_mean\", \"model.features.8.conv.3.running_var\", \"model.features.9.conv.0.0.weight\", \"model.features.9.conv.0.1.weight\", \"model.features.9.conv.0.1.bias\", \"model.features.9.conv.0.1.running_mean\", \"model.features.9.conv.0.1.running_var\", \"model.features.9.conv.1.0.weight\", \"model.features.9.conv.1.1.weight\", \"model.features.9.conv.1.1.bias\", \"model.features.9.conv.1.1.running_mean\", \"model.features.9.conv.1.1.running_var\", \"model.features.9.conv.2.weight\", \"model.features.9.conv.3.weight\", \"model.features.9.conv.3.bias\", \"model.features.9.conv.3.running_mean\", \"model.features.9.conv.3.running_var\", \"model.features.10.conv.0.0.weight\", \"model.features.10.conv.0.1.weight\", \"model.features.10.conv.0.1.bias\", \"model.features.10.conv.0.1.running_mean\", \"model.features.10.conv.0.1.running_var\", \"model.features.10.conv.1.0.weight\", \"model.features.10.conv.1.1.weight\", \"model.features.10.conv.1.1.bias\", \"model.features.10.conv.1.1.running_mean\", \"model.features.10.conv.1.1.running_var\", \"model.features.10.conv.2.weight\", \"model.features.10.conv.3.weight\", \"model.features.10.conv.3.bias\", \"model.features.10.conv.3.running_mean\", \"model.features.10.conv.3.running_var\", \"model.features.11.conv.0.0.weight\", \"model.features.11.conv.0.1.weight\", \"model.features.11.conv.0.1.bias\", \"model.features.11.conv.0.1.running_mean\", \"model.features.11.conv.0.1.running_var\", \"model.features.11.conv.1.0.weight\", \"model.features.11.conv.1.1.weight\", \"model.features.11.conv.1.1.bias\", \"model.features.11.conv.1.1.running_mean\", \"model.features.11.conv.1.1.running_var\", \"model.features.11.conv.2.weight\", \"model.features.11.conv.3.weight\", \"model.features.11.conv.3.bias\", \"model.features.11.conv.3.running_mean\", \"model.features.11.conv.3.running_var\", \"model.features.12.conv.0.0.weight\", \"model.features.12.conv.0.1.weight\", \"model.features.12.conv.0.1.bias\", \"model.features.12.conv.0.1.running_mean\", \"model.features.12.conv.0.1.running_var\", \"model.features.12.conv.1.0.weight\", \"model.features.12.conv.1.1.weight\", \"model.features.12.conv.1.1.bias\", \"model.features.12.conv.1.1.running_mean\", \"model.features.12.conv.1.1.running_var\", \"model.features.12.conv.2.weight\", \"model.features.12.conv.3.weight\", \"model.features.12.conv.3.bias\", \"model.features.12.conv.3.running_mean\", \"model.features.12.conv.3.running_var\", \"model.features.13.conv.0.0.weight\", \"model.features.13.conv.0.1.weight\", \"model.features.13.conv.0.1.bias\", \"model.features.13.conv.0.1.running_mean\", \"model.features.13.conv.0.1.running_var\", \"model.features.13.conv.1.0.weight\", \"model.features.13.conv.1.1.weight\", \"model.features.13.conv.1.1.bias\", \"model.features.13.conv.1.1.running_mean\", \"model.features.13.conv.1.1.running_var\", \"model.features.13.conv.2.weight\", \"model.features.13.conv.3.weight\", \"model.features.13.conv.3.bias\", \"model.features.13.conv.3.running_mean\", \"model.features.13.conv.3.running_var\", \"model.features.14.conv.0.0.weight\", \"model.features.14.conv.0.1.weight\", \"model.features.14.conv.0.1.bias\", \"model.features.14.conv.0.1.running_mean\", \"model.features.14.conv.0.1.running_var\", \"model.features.14.conv.1.0.weight\", \"model.features.14.conv.1.1.weight\", \"model.features.14.conv.1.1.bias\", \"model.features.14.conv.1.1.running_mean\", \"model.features.14.conv.1.1.running_var\", \"model.features.14.conv.2.weight\", \"model.features.14.conv.3.weight\", \"model.features.14.conv.3.bias\", \"model.features.14.conv.3.running_mean\", \"model.features.14.conv.3.running_var\", \"model.features.15.conv.0.0.weight\", \"model.features.15.conv.0.1.weight\", \"model.features.15.conv.0.1.bias\", \"model.features.15.conv.0.1.running_mean\", \"model.features.15.conv.0.1.running_var\", \"model.features.15.conv.1.0.weight\", \"model.features.15.conv.1.1.weight\", \"model.features.15.conv.1.1.bias\", \"model.features.15.conv.1.1.running_mean\", \"model.features.15.conv.1.1.running_var\", \"model.features.15.conv.2.weight\", \"model.features.15.conv.3.weight\", \"model.features.15.conv.3.bias\", \"model.features.15.conv.3.running_mean\", \"model.features.15.conv.3.running_var\", \"model.features.16.conv.0.0.weight\", \"model.features.16.conv.0.1.weight\", \"model.features.16.conv.0.1.bias\", \"model.features.16.conv.0.1.running_mean\", \"model.features.16.conv.0.1.running_var\", \"model.features.16.conv.1.0.weight\", \"model.features.16.conv.1.1.weight\", \"model.features.16.conv.1.1.bias\", \"model.features.16.conv.1.1.running_mean\", \"model.features.16.conv.1.1.running_var\", \"model.features.16.conv.2.weight\", \"model.features.16.conv.3.weight\", \"model.features.16.conv.3.bias\", \"model.features.16.conv.3.running_mean\", \"model.features.16.conv.3.running_var\", \"model.features.17.conv.0.0.weight\", \"model.features.17.conv.0.1.weight\", \"model.features.17.conv.0.1.bias\", \"model.features.17.conv.0.1.running_mean\", \"model.features.17.conv.0.1.running_var\", \"model.features.17.conv.1.0.weight\", \"model.features.17.conv.1.1.weight\", \"model.features.17.conv.1.1.bias\", \"model.features.17.conv.1.1.running_mean\", \"model.features.17.conv.1.1.running_var\", \"model.features.17.conv.2.weight\", \"model.features.17.conv.3.weight\", \"model.features.17.conv.3.bias\", \"model.features.17.conv.3.running_mean\", \"model.features.17.conv.3.running_var\", \"model.features.18.0.weight\", \"model.features.18.1.weight\", \"model.features.18.1.bias\", \"model.features.18.1.running_mean\", \"model.features.18.1.running_var\", \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n\tUnexpected key(s) in state_dict: \"pretrained.features.0.0.weight\", \"pretrained.features.0.1.weight\", \"pretrained.features.0.1.bias\", \"pretrained.features.0.1.running_mean\", \"pretrained.features.0.1.running_var\", \"pretrained.features.0.1.num_batches_tracked\", \"pretrained.features.1.conv.0.0.weight\", \"pretrained.features.1.conv.0.1.weight\", \"pretrained.features.1.conv.0.1.bias\", \"pretrained.features.1.conv.0.1.running_mean\", \"pretrained.features.1.conv.0.1.running_var\", \"pretrained.features.1.conv.0.1.num_batches_tracked\", \"pretrained.features.1.conv.1.weight\", \"pretrained.features.1.conv.2.weight\", \"pretrained.features.1.conv.2.bias\", \"pretrained.features.1.conv.2.running_mean\", \"pretrained.features.1.conv.2.running_var\", \"pretrained.features.1.conv.2.num_batches_tracked\", \"pretrained.features.2.conv.0.0.weight\", \"pretrained.features.2.conv.0.1.weight\", \"pretrained.features.2.conv.0.1.bias\", \"pretrained.features.2.conv.0.1.running_mean\", \"pretrained.features.2.conv.0.1.running_var\", \"pretrained.features.2.conv.0.1.num_batches_tracked\", \"pretrained.features.2.conv.1.0.weight\", \"pretrained.features.2.conv.1.1.weight\", \"pretrained.features.2.conv.1.1.bias\", \"pretrained.features.2.conv.1.1.running_mean\", \"pretrained.features.2.conv.1.1.running_var\", \"pretrained.features.2.conv.1.1.num_batches_tracked\", \"pretrained.features.2.conv.2.weight\", \"pretrained.features.2.conv.3.weight\", \"pretrained.features.2.conv.3.bias\", \"pretrained.features.2.conv.3.running_mean\", \"pretrained.features.2.conv.3.running_var\", \"pretrained.features.2.conv.3.num_batches_tracked\", \"pretrained.features.3.conv.0.0.weight\", \"pretrained.features.3.conv.0.1.weight\", \"pretrained.features.3.conv.0.1.bias\", \"pretrained.features.3.conv.0.1.running_mean\", \"pretrained.features.3.conv.0.1.running_var\", \"pretrained.features.3.conv.0.1.num_batches_tracked\", \"pretrained.features.3.conv.1.0.weight\", \"pretrained.features.3.conv.1.1.weight\", \"pretrained.features.3.conv.1.1.bias\", \"pretrained.features.3.conv.1.1.running_mean\", \"pretrained.features.3.conv.1.1.running_var\", \"pretrained.features.3.conv.1.1.num_batches_tracked\", \"pretrained.features.3.conv.2.weight\", \"pretrained.features.3.conv.3.weight\", \"pretrained.features.3.conv.3.bias\", \"pretrained.features.3.conv.3.running_mean\", \"pretrained.features.3.conv.3.running_var\", \"pretrained.features.3.conv.3.num_batches_tracked\", \"pretrained.features.4.conv.0.0.weight\", \"pretrained.features.4.conv.0.1.weight\", \"pretrained.features.4.conv.0.1.bias\", \"pretrained.features.4.conv.0.1.running_mean\", \"pretrained.features.4.conv.0.1.running_var\", \"pretrained.features.4.conv.0.1.num_batches_tracked\", \"pretrained.features.4.conv.1.0.weight\", \"pretrained.features.4.conv.1.1.weight\", \"pretrained.features.4.conv.1.1.bias\", \"pretrained.features.4.conv.1.1.running_mean\", \"pretrained.features.4.conv.1.1.running_var\", \"pretrained.features.4.conv.1.1.num_batches_tracked\", \"pretrained.features.4.conv.2.weight\", \"pretrained.features.4.conv.3.weight\", \"pretrained.features.4.conv.3.bias\", \"pretrained.features.4.conv.3.running_mean\", \"pretrained.features.4.conv.3.running_var\", \"pretrained.features.4.conv.3.num_batches_tracked\", \"pretrained.features.5.conv.0.0.weight\", \"pretrained.features.5.conv.0.1.weight\", \"pretrained.features.5.conv.0.1.bias\", \"pretrained.features.5.conv.0.1.running_mean\", \"pretrained.features.5.conv.0.1.running_var\", \"pretrained.features.5.conv.0.1.num_batches_tracked\", \"pretrained.features.5.conv.1.0.weight\", \"pretrained.features.5.conv.1.1.weight\", \"pretrained.features.5.conv.1.1.bias\", \"pretrained.features.5.conv.1.1.running_mean\", \"pretrained.features.5.conv.1.1.running_var\", \"pretrained.features.5.conv.1.1.num_batches_tracked\", \"pretrained.features.5.conv.2.weight\", \"pretrained.features.5.conv.3.weight\", \"pretrained.features.5.conv.3.bias\", \"pretrained.features.5.conv.3.running_mean\", \"pretrained.features.5.conv.3.running_var\", \"pretrained.features.5.conv.3.num_batches_tracked\", \"pretrained.features.6.conv.0.0.weight\", \"pretrained.features.6.conv.0.1.weight\", \"pretrained.features.6.conv.0.1.bias\", \"pretrained.features.6.conv.0.1.running_mean\", \"pretrained.features.6.conv.0.1.running_var\", \"pretrained.features.6.conv.0.1.num_batches_tracked\", \"pretrained.features.6.conv.1.0.weight\", \"pretrained.features.6.conv.1.1.weight\", \"pretrained.features.6.conv.1.1.bias\", \"pretrained.features.6.conv.1.1.running_mean\", \"pretrained.features.6.conv.1.1.running_var\", \"pretrained.features.6.conv.1.1.num_batches_tracked\", \"pretrained.features.6.conv.2.weight\", \"pretrained.features.6.conv.3.weight\", \"pretrained.features.6.conv.3.bias\", \"pretrained.features.6.conv.3.running_mean\", \"pretrained.features.6.conv.3.running_var\", \"pretrained.features.6.conv.3.num_batches_tracked\", \"pretrained.features.7.conv.0.0.weight\", \"pretrained.features.7.conv.0.1.weight\", \"pretrained.features.7.conv.0.1.bias\", \"pretrained.features.7.conv.0.1.running_mean\", \"pretrained.features.7.conv.0.1.running_var\", \"pretrained.features.7.conv.0.1.num_batches_tracked\", \"pretrained.features.7.conv.1.0.weight\", \"pretrained.features.7.conv.1.1.weight\", \"pretrained.features.7.conv.1.1.bias\", \"pretrained.features.7.conv.1.1.running_mean\", \"pretrained.features.7.conv.1.1.running_var\", \"pretrained.features.7.conv.1.1.num_batches_tracked\", \"pretrained.features.7.conv.2.weight\", \"pretrained.features.7.conv.3.weight\", \"pretrained.features.7.conv.3.bias\", \"pretrained.features.7.conv.3.running_mean\", \"pretrained.features.7.conv.3.running_var\", \"pretrained.features.7.conv.3.num_batches_tracked\", \"pretrained.features.8.conv.0.0.weight\", \"pretrained.features.8.conv.0.1.weight\", \"pretrained.features.8.conv.0.1.bias\", \"pretrained.features.8.conv.0.1.running_mean\", \"pretrained.features.8.conv.0.1.running_var\", \"pretrained.features.8.conv.0.1.num_batches_tracked\", \"pretrained.features.8.conv.1.0.weight\", \"pretrained.features.8.conv.1.1.weight\", \"pretrained.features.8.conv.1.1.bias\", \"pretrained.features.8.conv.1.1.running_mean\", \"pretrained.features.8.conv.1.1.running_var\", \"pretrained.features.8.conv.1.1.num_batches_tracked\", \"pretrained.features.8.conv.2.weight\", \"pretrained.features.8.conv.3.weight\", \"pretrained.features.8.conv.3.bias\", \"pretrained.features.8.conv.3.running_mean\", \"pretrained.features.8.conv.3.running_var\", \"pretrained.features.8.conv.3.num_batches_tracked\", \"pretrained.features.9.conv.0.0.weight\", \"pretrained.features.9.conv.0.1.weight\", \"pretrained.features.9.conv.0.1.bias\", \"pretrained.features.9.conv.0.1.running_mean\", \"pretrained.features.9.conv.0.1.running_var\", \"pretrained.features.9.conv.0.1.num_batches_tracked\", \"pretrained.features.9.conv.1.0.weight\", \"pretrained.features.9.conv.1.1.weight\", \"pretrained.features.9.conv.1.1.bias\", \"pretrained.features.9.conv.1.1.running_mean\", \"pretrained.features.9.conv.1.1.running_var\", \"pretrained.features.9.conv.1.1.num_batches_tracked\", \"pretrained.features.9.conv.2.weight\", \"pretrained.features.9.conv.3.weight\", \"pretrained.features.9.conv.3.bias\", \"pretrained.features.9.conv.3.running_mean\", \"pretrained.features.9.conv.3.running_var\", \"pretrained.features.9.conv.3.num_batches_tracked\", \"pretrained.features.10.conv.0.0.weight\", \"pretrained.features.10.conv.0.1.weight\", \"pretrained.features.10.conv.0.1.bias\", \"pretrained.features.10.conv.0.1.running_mean\", \"pretrained.features.10.conv.0.1.running_var\", \"pretrained.features.10.conv.0.1.num_batches_tracked\", \"pretrained.features.10.conv.1.0.weight\", \"pretrained.features.10.conv.1.1.weight\", \"pretrained.features.10.conv.1.1.bias\", \"pretrained.features.10.conv.1.1.running_mean\", \"pretrained.features.10.conv.1.1.running_var\", \"pretrained.features.10.conv.1.1.num_batches_tracked\", \"pretrained.features.10.conv.2.weight\", \"pretrained.features.10.conv.3.weight\", \"pretrained.features.10.conv.3.bias\", \"pretrained.features.10.conv.3.running_mean\", \"pretrained.features.10.conv.3.running_var\", \"pretrained.features.10.conv.3.num_batches_tracked\", \"pretrained.features.11.conv.0.0.weight\", \"pretrained.features.11.conv.0.1.weight\", \"pretrained.features.11.conv.0.1.bias\", \"pretrained.features.11.conv.0.1.running_mean\", \"pretrained.features.11.conv.0.1.running_var\", \"pretrained.features.11.conv.0.1.num_batches_tracked\", \"pretrained.features.11.conv.1.0.weight\", \"pretrained.features.11.conv.1.1.weight\", \"pretrained.features.11.conv.1.1.bias\", \"pretrained.features.11.conv.1.1.running_mean\", \"pretrained.features.11.conv.1.1.running_var\", \"pretrained.features.11.conv.1.1.num_batches_tracked\", \"pretrained.features.11.conv.2.weight\", \"pretrained.features.11.conv.3.weight\", \"pretrained.features.11.conv.3.bias\", \"pretrained.features.11.conv.3.running_mean\", \"pretrained.features.11.conv.3.running_var\", \"pretrained.features.11.conv.3.num_batches_tracked\", \"pretrained.features.12.conv.0.0.weight\", \"pretrained.features.12.conv.0.1.weight\", \"pretrained.features.12.conv.0.1.bias\", \"pretrained.features.12.conv.0.1.running_mean\", \"pretrained.features.12.conv.0.1.running_var\", \"pretrained.features.12.conv.0.1.num_batches_tracked\", \"pretrained.features.12.conv.1.0.weight\", \"pretrained.features.12.conv.1.1.weight\", \"pretrained.features.12.conv.1.1.bias\", \"pretrained.features.12.conv.1.1.running_mean\", \"pretrained.features.12.conv.1.1.running_var\", \"pretrained.features.12.conv.1.1.num_batches_tracked\", \"pretrained.features.12.conv.2.weight\", \"pretrained.features.12.conv.3.weight\", \"pretrained.features.12.conv.3.bias\", \"pretrained.features.12.conv.3.running_mean\", \"pretrained.features.12.conv.3.running_var\", \"pretrained.features.12.conv.3.num_batches_tracked\", \"pretrained.features.13.conv.0.0.weight\", \"pretrained.features.13.conv.0.1.weight\", \"pretrained.features.13.conv.0.1.bias\", \"pretrained.features.13.conv.0.1.running_mean\", \"pretrained.features.13.conv.0.1.running_var\", \"pretrained.features.13.conv.0.1.num_batches_tracked\", \"pretrained.features.13.conv.1.0.weight\", \"pretrained.features.13.conv.1.1.weight\", \"pretrained.features.13.conv.1.1.bias\", \"pretrained.features.13.conv.1.1.running_mean\", \"pretrained.features.13.conv.1.1.running_var\", \"pretrained.features.13.conv.1.1.num_batches_tracked\", \"pretrained.features.13.conv.2.weight\", \"pretrained.features.13.conv.3.weight\", \"pretrained.features.13.conv.3.bias\", \"pretrained.features.13.conv.3.running_mean\", \"pretrained.features.13.conv.3.running_var\", \"pretrained.features.13.conv.3.num_batches_tracked\", \"pretrained.features.14.conv.0.0.weight\", \"pretrained.features.14.conv.0.1.weight\", \"pretrained.features.14.conv.0.1.bias\", \"pretrained.features.14.conv.0.1.running_mean\", \"pretrained.features.14.conv.0.1.running_var\", \"pretrained.features.14.conv.0.1.num_batches_tracked\", \"pretrained.features.14.conv.1.0.weight\", \"pretrained.features.14.conv.1.1.weight\", \"pretrained.features.14.conv.1.1.bias\", \"pretrained.features.14.conv.1.1.running_mean\", \"pretrained.features.14.conv.1.1.running_var\", \"pretrained.features.14.conv.1.1.num_batches_tracked\", \"pretrained.features.14.conv.2.weight\", \"pretrained.features.14.conv.3.weight\", \"pretrained.features.14.conv.3.bias\", \"pretrained.features.14.conv.3.running_mean\", \"pretrained.features.14.conv.3.running_var\", \"pretrained.features.14.conv.3.num_batches_tracked\", \"pretrained.features.15.conv.0.0.weight\", \"pretrained.features.15.conv.0.1.weight\", \"pretrained.features.15.conv.0.1.bias\", \"pretrained.features.15.conv.0.1.running_mean\", \"pretrained.features.15.conv.0.1.running_var\", \"pretrained.features.15.conv.0.1.num_batches_tracked\", \"pretrained.features.15.conv.1.0.weight\", \"pretrained.features.15.conv.1.1.weight\", \"pretrained.features.15.conv.1.1.bias\", \"pretrained.features.15.conv.1.1.running_mean\", \"pretrained.features.15.conv.1.1.running_var\", \"pretrained.features.15.conv.1.1.num_batches_tracked\", \"pretrained.features.15.conv.2.weight\", \"pretrained.features.15.conv.3.weight\", \"pretrained.features.15.conv.3.bias\", \"pretrained.features.15.conv.3.running_mean\", \"pretrained.features.15.conv.3.running_var\", \"pretrained.features.15.conv.3.num_batches_tracked\", \"pretrained.features.16.conv.0.0.weight\", \"pretrained.features.16.conv.0.1.weight\", \"pretrained.features.16.conv.0.1.bias\", \"pretrained.features.16.conv.0.1.running_mean\", \"pretrained.features.16.conv.0.1.running_var\", \"pretrained.features.16.conv.0.1.num_batches_tracked\", \"pretrained.features.16.conv.1.0.weight\", \"pretrained.features.16.conv.1.1.weight\", \"pretrained.features.16.conv.1.1.bias\", \"pretrained.features.16.conv.1.1.running_mean\", \"pretrained.features.16.conv.1.1.running_var\", \"pretrained.features.16.conv.1.1.num_batches_tracked\", \"pretrained.features.16.conv.2.weight\", \"pretrained.features.16.conv.3.weight\", \"pretrained.features.16.conv.3.bias\", \"pretrained.features.16.conv.3.running_mean\", \"pretrained.features.16.conv.3.running_var\", \"pretrained.features.16.conv.3.num_batches_tracked\", \"pretrained.features.17.conv.0.0.weight\", \"pretrained.features.17.conv.0.1.weight\", \"pretrained.features.17.conv.0.1.bias\", \"pretrained.features.17.conv.0.1.running_mean\", \"pretrained.features.17.conv.0.1.running_var\", \"pretrained.features.17.conv.0.1.num_batches_tracked\", \"pretrained.features.17.conv.1.0.weight\", \"pretrained.features.17.conv.1.1.weight\", \"pretrained.features.17.conv.1.1.bias\", \"pretrained.features.17.conv.1.1.running_mean\", \"pretrained.features.17.conv.1.1.running_var\", \"pretrained.features.17.conv.1.1.num_batches_tracked\", \"pretrained.features.17.conv.2.weight\", \"pretrained.features.17.conv.3.weight\", \"pretrained.features.17.conv.3.bias\", \"pretrained.features.17.conv.3.running_mean\", \"pretrained.features.17.conv.3.running_var\", \"pretrained.features.17.conv.3.num_batches_tracked\", \"pretrained.features.18.0.weight\", \"pretrained.features.18.1.weight\", \"pretrained.features.18.1.bias\", \"pretrained.features.18.1.running_mean\", \"pretrained.features.18.1.running_var\", \"pretrained.features.18.1.num_batches_tracked\", \"pretrained.classifier.1.weight\", \"pretrained.classifier.1.bias\", \"my_new_layers.0.weight\", \"my_new_layers.0.bias\", \"my_new_layers.3.weight\", \"my_new_layers.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m MobileNet(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     23\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load và xử lý ảnh\u001b[39;00m\n",
      "File \u001b[1;32mh:\\AnacondaApp\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNet:\n\tMissing key(s) in state_dict: \"model.features.0.0.weight\", \"model.features.0.1.weight\", \"model.features.0.1.bias\", \"model.features.0.1.running_mean\", \"model.features.0.1.running_var\", \"model.features.1.conv.0.0.weight\", \"model.features.1.conv.0.1.weight\", \"model.features.1.conv.0.1.bias\", \"model.features.1.conv.0.1.running_mean\", \"model.features.1.conv.0.1.running_var\", \"model.features.1.conv.1.weight\", \"model.features.1.conv.2.weight\", \"model.features.1.conv.2.bias\", \"model.features.1.conv.2.running_mean\", \"model.features.1.conv.2.running_var\", \"model.features.2.conv.0.0.weight\", \"model.features.2.conv.0.1.weight\", \"model.features.2.conv.0.1.bias\", \"model.features.2.conv.0.1.running_mean\", \"model.features.2.conv.0.1.running_var\", \"model.features.2.conv.1.0.weight\", \"model.features.2.conv.1.1.weight\", \"model.features.2.conv.1.1.bias\", \"model.features.2.conv.1.1.running_mean\", \"model.features.2.conv.1.1.running_var\", \"model.features.2.conv.2.weight\", \"model.features.2.conv.3.weight\", \"model.features.2.conv.3.bias\", \"model.features.2.conv.3.running_mean\", \"model.features.2.conv.3.running_var\", \"model.features.3.conv.0.0.weight\", \"model.features.3.conv.0.1.weight\", \"model.features.3.conv.0.1.bias\", \"model.features.3.conv.0.1.running_mean\", \"model.features.3.conv.0.1.running_var\", \"model.features.3.conv.1.0.weight\", \"model.features.3.conv.1.1.weight\", \"model.features.3.conv.1.1.bias\", \"model.features.3.conv.1.1.running_mean\", \"model.features.3.conv.1.1.running_var\", \"model.features.3.conv.2.weight\", \"model.features.3.conv.3.weight\", \"model.features.3.conv.3.bias\", \"model.features.3.conv.3.running_mean\", \"model.features.3.conv.3.running_var\", \"model.features.4.conv.0.0.weight\", \"model.features.4.conv.0.1.weight\", \"model.features.4.conv.0.1.bias\", \"model.features.4.conv.0.1.running_mean\", \"model.features.4.conv.0.1.running_var\", \"model.features.4.conv.1.0.weight\", \"model.features.4.conv.1.1.weight\", \"model.features.4.conv.1.1.bias\", \"model.features.4.conv.1.1.running_mean\", \"model.features.4.conv.1.1.running_var\", \"model.features.4.conv.2.weight\", \"model.features.4.conv.3.weight\", \"model.features.4.conv.3.bias\", \"model.features.4.conv.3.running_mean\", \"model.features.4.conv.3.running_var\", \"model.features.5.conv.0.0.weight\", \"model.features.5.conv.0.1.weight\", \"model.features.5.conv.0.1.bias\", \"model.features.5.conv.0.1.running_mean\", \"model.features.5.conv.0.1.running_var\", \"model.features.5.conv.1.0.weight\", \"model.features.5.conv.1.1.weight\", \"model.features.5.conv.1.1.bias\", \"model.features.5.conv.1.1.running_mean\", \"model.features.5.conv.1.1.running_var\", \"model.features.5.conv.2.weight\", \"model.features.5.conv.3.weight\", \"model.features.5.conv.3.bias\", \"model.features.5.conv.3.running_mean\", \"model.features.5.conv.3.running_var\", \"model.features.6.conv.0.0.weight\", \"model.features.6.conv.0.1.weight\", \"model.features.6.conv.0.1.bias\", \"model.features.6.conv.0.1.running_mean\", \"model.features.6.conv.0.1.running_var\", \"model.features.6.conv.1.0.weight\", \"model.features.6.conv.1.1.weight\", \"model.features.6.conv.1.1.bias\", \"model.features.6.conv.1.1.running_mean\", \"model.features.6.conv.1.1.running_var\", \"model.features.6.conv.2.weight\", \"model.features.6.conv.3.weight\", \"model.features.6.conv.3.bias\", \"model.features.6.conv.3.running_mean\", \"model.features.6.conv.3.running_var\", \"model.features.7.conv.0.0.weight\", \"model.features.7.conv.0.1.weight\", \"model.features.7.conv.0.1.bias\", \"model.features.7.conv.0.1.running_mean\", \"model.features.7.conv.0.1.running_var\", \"model.features.7.conv.1.0.weight\", \"model.features.7.conv.1.1.weight\", \"model.features.7.conv.1.1.bias\", \"model.features.7.conv.1.1.running_mean\", \"model.features.7.conv.1.1.running_var\", \"model.features.7.conv.2.weight\", \"model.features.7.conv.3.weight\", \"model.features.7.conv.3.bias\", \"model.features.7.conv.3.running_mean\", \"model.features.7.conv.3.running_var\", \"model.features.8.conv.0.0.weight\", \"model.features.8.conv.0.1.weight\", \"model.features.8.conv.0.1.bias\", \"model.features.8.conv.0.1.running_mean\", \"model.features.8.conv.0.1.running_var\", \"model.features.8.conv.1.0.weight\", \"model.features.8.conv.1.1.weight\", \"model.features.8.conv.1.1.bias\", \"model.features.8.conv.1.1.running_mean\", \"model.features.8.conv.1.1.running_var\", \"model.features.8.conv.2.weight\", \"model.features.8.conv.3.weight\", \"model.features.8.conv.3.bias\", \"model.features.8.conv.3.running_mean\", \"model.features.8.conv.3.running_var\", \"model.features.9.conv.0.0.weight\", \"model.features.9.conv.0.1.weight\", \"model.features.9.conv.0.1.bias\", \"model.features.9.conv.0.1.running_mean\", \"model.features.9.conv.0.1.running_var\", \"model.features.9.conv.1.0.weight\", \"model.features.9.conv.1.1.weight\", \"model.features.9.conv.1.1.bias\", \"model.features.9.conv.1.1.running_mean\", \"model.features.9.conv.1.1.running_var\", \"model.features.9.conv.2.weight\", \"model.features.9.conv.3.weight\", \"model.features.9.conv.3.bias\", \"model.features.9.conv.3.running_mean\", \"model.features.9.conv.3.running_var\", \"model.features.10.conv.0.0.weight\", \"model.features.10.conv.0.1.weight\", \"model.features.10.conv.0.1.bias\", \"model.features.10.conv.0.1.running_mean\", \"model.features.10.conv.0.1.running_var\", \"model.features.10.conv.1.0.weight\", \"model.features.10.conv.1.1.weight\", \"model.features.10.conv.1.1.bias\", \"model.features.10.conv.1.1.running_mean\", \"model.features.10.conv.1.1.running_var\", \"model.features.10.conv.2.weight\", \"model.features.10.conv.3.weight\", \"model.features.10.conv.3.bias\", \"model.features.10.conv.3.running_mean\", \"model.features.10.conv.3.running_var\", \"model.features.11.conv.0.0.weight\", \"model.features.11.conv.0.1.weight\", \"model.features.11.conv.0.1.bias\", \"model.features.11.conv.0.1.running_mean\", \"model.features.11.conv.0.1.running_var\", \"model.features.11.conv.1.0.weight\", \"model.features.11.conv.1.1.weight\", \"model.features.11.conv.1.1.bias\", \"model.features.11.conv.1.1.running_mean\", \"model.features.11.conv.1.1.running_var\", \"model.features.11.conv.2.weight\", \"model.features.11.conv.3.weight\", \"model.features.11.conv.3.bias\", \"model.features.11.conv.3.running_mean\", \"model.features.11.conv.3.running_var\", \"model.features.12.conv.0.0.weight\", \"model.features.12.conv.0.1.weight\", \"model.features.12.conv.0.1.bias\", \"model.features.12.conv.0.1.running_mean\", \"model.features.12.conv.0.1.running_var\", \"model.features.12.conv.1.0.weight\", \"model.features.12.conv.1.1.weight\", \"model.features.12.conv.1.1.bias\", \"model.features.12.conv.1.1.running_mean\", \"model.features.12.conv.1.1.running_var\", \"model.features.12.conv.2.weight\", \"model.features.12.conv.3.weight\", \"model.features.12.conv.3.bias\", \"model.features.12.conv.3.running_mean\", \"model.features.12.conv.3.running_var\", \"model.features.13.conv.0.0.weight\", \"model.features.13.conv.0.1.weight\", \"model.features.13.conv.0.1.bias\", \"model.features.13.conv.0.1.running_mean\", \"model.features.13.conv.0.1.running_var\", \"model.features.13.conv.1.0.weight\", \"model.features.13.conv.1.1.weight\", \"model.features.13.conv.1.1.bias\", \"model.features.13.conv.1.1.running_mean\", \"model.features.13.conv.1.1.running_var\", \"model.features.13.conv.2.weight\", \"model.features.13.conv.3.weight\", \"model.features.13.conv.3.bias\", \"model.features.13.conv.3.running_mean\", \"model.features.13.conv.3.running_var\", \"model.features.14.conv.0.0.weight\", \"model.features.14.conv.0.1.weight\", \"model.features.14.conv.0.1.bias\", \"model.features.14.conv.0.1.running_mean\", \"model.features.14.conv.0.1.running_var\", \"model.features.14.conv.1.0.weight\", \"model.features.14.conv.1.1.weight\", \"model.features.14.conv.1.1.bias\", \"model.features.14.conv.1.1.running_mean\", \"model.features.14.conv.1.1.running_var\", \"model.features.14.conv.2.weight\", \"model.features.14.conv.3.weight\", \"model.features.14.conv.3.bias\", \"model.features.14.conv.3.running_mean\", \"model.features.14.conv.3.running_var\", \"model.features.15.conv.0.0.weight\", \"model.features.15.conv.0.1.weight\", \"model.features.15.conv.0.1.bias\", \"model.features.15.conv.0.1.running_mean\", \"model.features.15.conv.0.1.running_var\", \"model.features.15.conv.1.0.weight\", \"model.features.15.conv.1.1.weight\", \"model.features.15.conv.1.1.bias\", \"model.features.15.conv.1.1.running_mean\", \"model.features.15.conv.1.1.running_var\", \"model.features.15.conv.2.weight\", \"model.features.15.conv.3.weight\", \"model.features.15.conv.3.bias\", \"model.features.15.conv.3.running_mean\", \"model.features.15.conv.3.running_var\", \"model.features.16.conv.0.0.weight\", \"model.features.16.conv.0.1.weight\", \"model.features.16.conv.0.1.bias\", \"model.features.16.conv.0.1.running_mean\", \"model.features.16.conv.0.1.running_var\", \"model.features.16.conv.1.0.weight\", \"model.features.16.conv.1.1.weight\", \"model.features.16.conv.1.1.bias\", \"model.features.16.conv.1.1.running_mean\", \"model.features.16.conv.1.1.running_var\", \"model.features.16.conv.2.weight\", \"model.features.16.conv.3.weight\", \"model.features.16.conv.3.bias\", \"model.features.16.conv.3.running_mean\", \"model.features.16.conv.3.running_var\", \"model.features.17.conv.0.0.weight\", \"model.features.17.conv.0.1.weight\", \"model.features.17.conv.0.1.bias\", \"model.features.17.conv.0.1.running_mean\", \"model.features.17.conv.0.1.running_var\", \"model.features.17.conv.1.0.weight\", \"model.features.17.conv.1.1.weight\", \"model.features.17.conv.1.1.bias\", \"model.features.17.conv.1.1.running_mean\", \"model.features.17.conv.1.1.running_var\", \"model.features.17.conv.2.weight\", \"model.features.17.conv.3.weight\", \"model.features.17.conv.3.bias\", \"model.features.17.conv.3.running_mean\", \"model.features.17.conv.3.running_var\", \"model.features.18.0.weight\", \"model.features.18.1.weight\", \"model.features.18.1.bias\", \"model.features.18.1.running_mean\", \"model.features.18.1.running_var\", \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n\tUnexpected key(s) in state_dict: \"pretrained.features.0.0.weight\", \"pretrained.features.0.1.weight\", \"pretrained.features.0.1.bias\", \"pretrained.features.0.1.running_mean\", \"pretrained.features.0.1.running_var\", \"pretrained.features.0.1.num_batches_tracked\", \"pretrained.features.1.conv.0.0.weight\", \"pretrained.features.1.conv.0.1.weight\", \"pretrained.features.1.conv.0.1.bias\", \"pretrained.features.1.conv.0.1.running_mean\", \"pretrained.features.1.conv.0.1.running_var\", \"pretrained.features.1.conv.0.1.num_batches_tracked\", \"pretrained.features.1.conv.1.weight\", \"pretrained.features.1.conv.2.weight\", \"pretrained.features.1.conv.2.bias\", \"pretrained.features.1.conv.2.running_mean\", \"pretrained.features.1.conv.2.running_var\", \"pretrained.features.1.conv.2.num_batches_tracked\", \"pretrained.features.2.conv.0.0.weight\", \"pretrained.features.2.conv.0.1.weight\", \"pretrained.features.2.conv.0.1.bias\", \"pretrained.features.2.conv.0.1.running_mean\", \"pretrained.features.2.conv.0.1.running_var\", \"pretrained.features.2.conv.0.1.num_batches_tracked\", \"pretrained.features.2.conv.1.0.weight\", \"pretrained.features.2.conv.1.1.weight\", \"pretrained.features.2.conv.1.1.bias\", \"pretrained.features.2.conv.1.1.running_mean\", \"pretrained.features.2.conv.1.1.running_var\", \"pretrained.features.2.conv.1.1.num_batches_tracked\", \"pretrained.features.2.conv.2.weight\", \"pretrained.features.2.conv.3.weight\", \"pretrained.features.2.conv.3.bias\", \"pretrained.features.2.conv.3.running_mean\", \"pretrained.features.2.conv.3.running_var\", \"pretrained.features.2.conv.3.num_batches_tracked\", \"pretrained.features.3.conv.0.0.weight\", \"pretrained.features.3.conv.0.1.weight\", \"pretrained.features.3.conv.0.1.bias\", \"pretrained.features.3.conv.0.1.running_mean\", \"pretrained.features.3.conv.0.1.running_var\", \"pretrained.features.3.conv.0.1.num_batches_tracked\", \"pretrained.features.3.conv.1.0.weight\", \"pretrained.features.3.conv.1.1.weight\", \"pretrained.features.3.conv.1.1.bias\", \"pretrained.features.3.conv.1.1.running_mean\", \"pretrained.features.3.conv.1.1.running_var\", \"pretrained.features.3.conv.1.1.num_batches_tracked\", \"pretrained.features.3.conv.2.weight\", \"pretrained.features.3.conv.3.weight\", \"pretrained.features.3.conv.3.bias\", \"pretrained.features.3.conv.3.running_mean\", \"pretrained.features.3.conv.3.running_var\", \"pretrained.features.3.conv.3.num_batches_tracked\", \"pretrained.features.4.conv.0.0.weight\", \"pretrained.features.4.conv.0.1.weight\", \"pretrained.features.4.conv.0.1.bias\", \"pretrained.features.4.conv.0.1.running_mean\", \"pretrained.features.4.conv.0.1.running_var\", \"pretrained.features.4.conv.0.1.num_batches_tracked\", \"pretrained.features.4.conv.1.0.weight\", \"pretrained.features.4.conv.1.1.weight\", \"pretrained.features.4.conv.1.1.bias\", \"pretrained.features.4.conv.1.1.running_mean\", \"pretrained.features.4.conv.1.1.running_var\", \"pretrained.features.4.conv.1.1.num_batches_tracked\", \"pretrained.features.4.conv.2.weight\", \"pretrained.features.4.conv.3.weight\", \"pretrained.features.4.conv.3.bias\", \"pretrained.features.4.conv.3.running_mean\", \"pretrained.features.4.conv.3.running_var\", \"pretrained.features.4.conv.3.num_batches_tracked\", \"pretrained.features.5.conv.0.0.weight\", \"pretrained.features.5.conv.0.1.weight\", \"pretrained.features.5.conv.0.1.bias\", \"pretrained.features.5.conv.0.1.running_mean\", \"pretrained.features.5.conv.0.1.running_var\", \"pretrained.features.5.conv.0.1.num_batches_tracked\", \"pretrained.features.5.conv.1.0.weight\", \"pretrained.features.5.conv.1.1.weight\", \"pretrained.features.5.conv.1.1.bias\", \"pretrained.features.5.conv.1.1.running_mean\", \"pretrained.features.5.conv.1.1.running_var\", \"pretrained.features.5.conv.1.1.num_batches_tracked\", \"pretrained.features.5.conv.2.weight\", \"pretrained.features.5.conv.3.weight\", \"pretrained.features.5.conv.3.bias\", \"pretrained.features.5.conv.3.running_mean\", \"pretrained.features.5.conv.3.running_var\", \"pretrained.features.5.conv.3.num_batches_tracked\", \"pretrained.features.6.conv.0.0.weight\", \"pretrained.features.6.conv.0.1.weight\", \"pretrained.features.6.conv.0.1.bias\", \"pretrained.features.6.conv.0.1.running_mean\", \"pretrained.features.6.conv.0.1.running_var\", \"pretrained.features.6.conv.0.1.num_batches_tracked\", \"pretrained.features.6.conv.1.0.weight\", \"pretrained.features.6.conv.1.1.weight\", \"pretrained.features.6.conv.1.1.bias\", \"pretrained.features.6.conv.1.1.running_mean\", \"pretrained.features.6.conv.1.1.running_var\", \"pretrained.features.6.conv.1.1.num_batches_tracked\", \"pretrained.features.6.conv.2.weight\", \"pretrained.features.6.conv.3.weight\", \"pretrained.features.6.conv.3.bias\", \"pretrained.features.6.conv.3.running_mean\", \"pretrained.features.6.conv.3.running_var\", \"pretrained.features.6.conv.3.num_batches_tracked\", \"pretrained.features.7.conv.0.0.weight\", \"pretrained.features.7.conv.0.1.weight\", \"pretrained.features.7.conv.0.1.bias\", \"pretrained.features.7.conv.0.1.running_mean\", \"pretrained.features.7.conv.0.1.running_var\", \"pretrained.features.7.conv.0.1.num_batches_tracked\", \"pretrained.features.7.conv.1.0.weight\", \"pretrained.features.7.conv.1.1.weight\", \"pretrained.features.7.conv.1.1.bias\", \"pretrained.features.7.conv.1.1.running_mean\", \"pretrained.features.7.conv.1.1.running_var\", \"pretrained.features.7.conv.1.1.num_batches_tracked\", \"pretrained.features.7.conv.2.weight\", \"pretrained.features.7.conv.3.weight\", \"pretrained.features.7.conv.3.bias\", \"pretrained.features.7.conv.3.running_mean\", \"pretrained.features.7.conv.3.running_var\", \"pretrained.features.7.conv.3.num_batches_tracked\", \"pretrained.features.8.conv.0.0.weight\", \"pretrained.features.8.conv.0.1.weight\", \"pretrained.features.8.conv.0.1.bias\", \"pretrained.features.8.conv.0.1.running_mean\", \"pretrained.features.8.conv.0.1.running_var\", \"pretrained.features.8.conv.0.1.num_batches_tracked\", \"pretrained.features.8.conv.1.0.weight\", \"pretrained.features.8.conv.1.1.weight\", \"pretrained.features.8.conv.1.1.bias\", \"pretrained.features.8.conv.1.1.running_mean\", \"pretrained.features.8.conv.1.1.running_var\", \"pretrained.features.8.conv.1.1.num_batches_tracked\", \"pretrained.features.8.conv.2.weight\", \"pretrained.features.8.conv.3.weight\", \"pretrained.features.8.conv.3.bias\", \"pretrained.features.8.conv.3.running_mean\", \"pretrained.features.8.conv.3.running_var\", \"pretrained.features.8.conv.3.num_batches_tracked\", \"pretrained.features.9.conv.0.0.weight\", \"pretrained.features.9.conv.0.1.weight\", \"pretrained.features.9.conv.0.1.bias\", \"pretrained.features.9.conv.0.1.running_mean\", \"pretrained.features.9.conv.0.1.running_var\", \"pretrained.features.9.conv.0.1.num_batches_tracked\", \"pretrained.features.9.conv.1.0.weight\", \"pretrained.features.9.conv.1.1.weight\", \"pretrained.features.9.conv.1.1.bias\", \"pretrained.features.9.conv.1.1.running_mean\", \"pretrained.features.9.conv.1.1.running_var\", \"pretrained.features.9.conv.1.1.num_batches_tracked\", \"pretrained.features.9.conv.2.weight\", \"pretrained.features.9.conv.3.weight\", \"pretrained.features.9.conv.3.bias\", \"pretrained.features.9.conv.3.running_mean\", \"pretrained.features.9.conv.3.running_var\", \"pretrained.features.9.conv.3.num_batches_tracked\", \"pretrained.features.10.conv.0.0.weight\", \"pretrained.features.10.conv.0.1.weight\", \"pretrained.features.10.conv.0.1.bias\", \"pretrained.features.10.conv.0.1.running_mean\", \"pretrained.features.10.conv.0.1.running_var\", \"pretrained.features.10.conv.0.1.num_batches_tracked\", \"pretrained.features.10.conv.1.0.weight\", \"pretrained.features.10.conv.1.1.weight\", \"pretrained.features.10.conv.1.1.bias\", \"pretrained.features.10.conv.1.1.running_mean\", \"pretrained.features.10.conv.1.1.running_var\", \"pretrained.features.10.conv.1.1.num_batches_tracked\", \"pretrained.features.10.conv.2.weight\", \"pretrained.features.10.conv.3.weight\", \"pretrained.features.10.conv.3.bias\", \"pretrained.features.10.conv.3.running_mean\", \"pretrained.features.10.conv.3.running_var\", \"pretrained.features.10.conv.3.num_batches_tracked\", \"pretrained.features.11.conv.0.0.weight\", \"pretrained.features.11.conv.0.1.weight\", \"pretrained.features.11.conv.0.1.bias\", \"pretrained.features.11.conv.0.1.running_mean\", \"pretrained.features.11.conv.0.1.running_var\", \"pretrained.features.11.conv.0.1.num_batches_tracked\", \"pretrained.features.11.conv.1.0.weight\", \"pretrained.features.11.conv.1.1.weight\", \"pretrained.features.11.conv.1.1.bias\", \"pretrained.features.11.conv.1.1.running_mean\", \"pretrained.features.11.conv.1.1.running_var\", \"pretrained.features.11.conv.1.1.num_batches_tracked\", \"pretrained.features.11.conv.2.weight\", \"pretrained.features.11.conv.3.weight\", \"pretrained.features.11.conv.3.bias\", \"pretrained.features.11.conv.3.running_mean\", \"pretrained.features.11.conv.3.running_var\", \"pretrained.features.11.conv.3.num_batches_tracked\", \"pretrained.features.12.conv.0.0.weight\", \"pretrained.features.12.conv.0.1.weight\", \"pretrained.features.12.conv.0.1.bias\", \"pretrained.features.12.conv.0.1.running_mean\", \"pretrained.features.12.conv.0.1.running_var\", \"pretrained.features.12.conv.0.1.num_batches_tracked\", \"pretrained.features.12.conv.1.0.weight\", \"pretrained.features.12.conv.1.1.weight\", \"pretrained.features.12.conv.1.1.bias\", \"pretrained.features.12.conv.1.1.running_mean\", \"pretrained.features.12.conv.1.1.running_var\", \"pretrained.features.12.conv.1.1.num_batches_tracked\", \"pretrained.features.12.conv.2.weight\", \"pretrained.features.12.conv.3.weight\", \"pretrained.features.12.conv.3.bias\", \"pretrained.features.12.conv.3.running_mean\", \"pretrained.features.12.conv.3.running_var\", \"pretrained.features.12.conv.3.num_batches_tracked\", \"pretrained.features.13.conv.0.0.weight\", \"pretrained.features.13.conv.0.1.weight\", \"pretrained.features.13.conv.0.1.bias\", \"pretrained.features.13.conv.0.1.running_mean\", \"pretrained.features.13.conv.0.1.running_var\", \"pretrained.features.13.conv.0.1.num_batches_tracked\", \"pretrained.features.13.conv.1.0.weight\", \"pretrained.features.13.conv.1.1.weight\", \"pretrained.features.13.conv.1.1.bias\", \"pretrained.features.13.conv.1.1.running_mean\", \"pretrained.features.13.conv.1.1.running_var\", \"pretrained.features.13.conv.1.1.num_batches_tracked\", \"pretrained.features.13.conv.2.weight\", \"pretrained.features.13.conv.3.weight\", \"pretrained.features.13.conv.3.bias\", \"pretrained.features.13.conv.3.running_mean\", \"pretrained.features.13.conv.3.running_var\", \"pretrained.features.13.conv.3.num_batches_tracked\", \"pretrained.features.14.conv.0.0.weight\", \"pretrained.features.14.conv.0.1.weight\", \"pretrained.features.14.conv.0.1.bias\", \"pretrained.features.14.conv.0.1.running_mean\", \"pretrained.features.14.conv.0.1.running_var\", \"pretrained.features.14.conv.0.1.num_batches_tracked\", \"pretrained.features.14.conv.1.0.weight\", \"pretrained.features.14.conv.1.1.weight\", \"pretrained.features.14.conv.1.1.bias\", \"pretrained.features.14.conv.1.1.running_mean\", \"pretrained.features.14.conv.1.1.running_var\", \"pretrained.features.14.conv.1.1.num_batches_tracked\", \"pretrained.features.14.conv.2.weight\", \"pretrained.features.14.conv.3.weight\", \"pretrained.features.14.conv.3.bias\", \"pretrained.features.14.conv.3.running_mean\", \"pretrained.features.14.conv.3.running_var\", \"pretrained.features.14.conv.3.num_batches_tracked\", \"pretrained.features.15.conv.0.0.weight\", \"pretrained.features.15.conv.0.1.weight\", \"pretrained.features.15.conv.0.1.bias\", \"pretrained.features.15.conv.0.1.running_mean\", \"pretrained.features.15.conv.0.1.running_var\", \"pretrained.features.15.conv.0.1.num_batches_tracked\", \"pretrained.features.15.conv.1.0.weight\", \"pretrained.features.15.conv.1.1.weight\", \"pretrained.features.15.conv.1.1.bias\", \"pretrained.features.15.conv.1.1.running_mean\", \"pretrained.features.15.conv.1.1.running_var\", \"pretrained.features.15.conv.1.1.num_batches_tracked\", \"pretrained.features.15.conv.2.weight\", \"pretrained.features.15.conv.3.weight\", \"pretrained.features.15.conv.3.bias\", \"pretrained.features.15.conv.3.running_mean\", \"pretrained.features.15.conv.3.running_var\", \"pretrained.features.15.conv.3.num_batches_tracked\", \"pretrained.features.16.conv.0.0.weight\", \"pretrained.features.16.conv.0.1.weight\", \"pretrained.features.16.conv.0.1.bias\", \"pretrained.features.16.conv.0.1.running_mean\", \"pretrained.features.16.conv.0.1.running_var\", \"pretrained.features.16.conv.0.1.num_batches_tracked\", \"pretrained.features.16.conv.1.0.weight\", \"pretrained.features.16.conv.1.1.weight\", \"pretrained.features.16.conv.1.1.bias\", \"pretrained.features.16.conv.1.1.running_mean\", \"pretrained.features.16.conv.1.1.running_var\", \"pretrained.features.16.conv.1.1.num_batches_tracked\", \"pretrained.features.16.conv.2.weight\", \"pretrained.features.16.conv.3.weight\", \"pretrained.features.16.conv.3.bias\", \"pretrained.features.16.conv.3.running_mean\", \"pretrained.features.16.conv.3.running_var\", \"pretrained.features.16.conv.3.num_batches_tracked\", \"pretrained.features.17.conv.0.0.weight\", \"pretrained.features.17.conv.0.1.weight\", \"pretrained.features.17.conv.0.1.bias\", \"pretrained.features.17.conv.0.1.running_mean\", \"pretrained.features.17.conv.0.1.running_var\", \"pretrained.features.17.conv.0.1.num_batches_tracked\", \"pretrained.features.17.conv.1.0.weight\", \"pretrained.features.17.conv.1.1.weight\", \"pretrained.features.17.conv.1.1.bias\", \"pretrained.features.17.conv.1.1.running_mean\", \"pretrained.features.17.conv.1.1.running_var\", \"pretrained.features.17.conv.1.1.num_batches_tracked\", \"pretrained.features.17.conv.2.weight\", \"pretrained.features.17.conv.3.weight\", \"pretrained.features.17.conv.3.bias\", \"pretrained.features.17.conv.3.running_mean\", \"pretrained.features.17.conv.3.running_var\", \"pretrained.features.17.conv.3.num_batches_tracked\", \"pretrained.features.18.0.weight\", \"pretrained.features.18.1.weight\", \"pretrained.features.18.1.bias\", \"pretrained.features.18.1.running_mean\", \"pretrained.features.18.1.running_var\", \"pretrained.features.18.1.num_batches_tracked\", \"pretrained.classifier.1.weight\", \"pretrained.classifier.1.bias\", \"my_new_layers.0.weight\", \"my_new_layers.0.bias\", \"my_new_layers.3.weight\", \"my_new_layers.3.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "# Đường dẫn đến mô hình và ảnh cần kiểm tra\n",
    "# Ở đây file model đó là đuôi .pth, ở đây đó là best.pth.tar ở experiments/six_classes/mobilenet_baseline/\n",
    "model_path = \"experiments/six_classes/mobilenet_baseline/best.pth.tar\"\n",
    "image_path = \"H:/Coffee dataset/Coffee leaf in lab/coffee-leaf-diseases/test/images/64.jpg\"\n",
    "\n",
    "# Thiết lập mô hình\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False)\n",
    "        self.model.classifier[1] = nn.Linear(in_features=self.model.classifier[1].in_features, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Tạo mô hình và load trọng số\n",
    "model = MobileNet(num_classes=6)\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Load và xử lý ảnh\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Dự đoán\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "# In kết quả\n",
    "labels = ['healthy', 'rust_level_1', 'rust_level_2', 'rust_level_3', 'rust_level_4', 'red_spider_mite']\n",
    "print(f'This leaf is predicted to have: {labels[predicted.item()]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
